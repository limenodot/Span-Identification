{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\OneDrive\\Documents\\ДИССЕРТАЦИЯ\\Span Identification Project\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "data = pd.read_csv('bert-base-uncased_train_embeddings_with_span_diff-sum.csv')['label'].values.reshape(-1, 1)\n",
    "onehot_encoder.fit(data)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, onehot_encoder):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        # self.embeddings = [torch.load(data.iloc[i]['path_to_embeddings']).cpu() for i in range(len(data))]\n",
    "        self.labels = onehot_encoder.transform(self.data['label'].values.reshape(-1, 1))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        file_path = self.data.iloc[idx]['path_to_embeddings']\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Load embedding from .pt file\n",
    "        embedding = torch.load(file_path)\n",
    "\n",
    "        # return embedding, torch.FloatTensor(label_onehot)\n",
    "        return embedding, torch.FloatTensor(label)\n",
    "    \n",
    "# Paths to train and test CSV files\n",
    "train_csv_path = 'bert-base-uncased_train_embeddings_with_span_raw.csv'\n",
    "test_csv_path = 'bert-base-uncased_test_embeddings_with_span_raw.csv'\n",
    "\n",
    "# Create instances of CustomDataset for train and test\n",
    "train_dataset = CustomDataset(train_csv_path, onehot_encoder)\n",
    "test_dataset = CustomDataset(test_csv_path, onehot_encoder)\n",
    "\n",
    "# Define batch sizes\n",
    "train_batch_size = 1\n",
    "test_batch_size = 1\n",
    "\n",
    "# Create DataLoaders for train and test datasets\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1536])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomDeepClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=1536, num_classes=20, num_layers=20, hidden_dim=1024):\n",
    "        super(CustomDeepClassifier, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.BatchNorm1d(1))\n",
    "\n",
    "        for _ in range(num_layers - 2):  # Subtract 2 for the initial and final layers\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.BatchNorm1d(1))\n",
    "\n",
    "        layers.append(nn.Linear(hidden_dim, num_classes))\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        x = F.softmax(x, dim=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_validate_model(model, train_loader, val_loader, num_epochs=10, learning_rate=0.1, stepslr=10, gamma=0.9):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, stepslr, gamma=gamma)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), total=num_epochs):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs.float())\n",
    "            # print(outputs.shape, labels.shape)\n",
    "            # print(outputs.squeeze(1), labels)\n",
    "            loss = criterion(outputs.squeeze(1), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item()\n",
    "        \n",
    "        # print(running_train_loss, len(train_loader))\n",
    "        train_loss = running_train_loss / len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            model.eval()\n",
    "            running_val_loss = 0.0\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in val_loader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs = model(inputs.float())\n",
    "                    loss = criterion(outputs.squeeze(1), labels)\n",
    "                    running_val_loss += loss.item()\n",
    "\n",
    "                    # print(outputs.shape, labels.shape)\n",
    "                    # print(outputs, labels)\n",
    "                    predicted = torch.argmax(outputs.squeeze(1), 1)\n",
    "                    labels = torch.argmax(labels, 1)\n",
    "                    # print(predicted.shape, labels.shape)\n",
    "                    # print(predicted, labels)\n",
    "                    # break\n",
    "                    all_preds.extend(predicted.cpu().numpy())\n",
    "                    all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            val_loss = running_val_loss / len(val_loader)\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='macro')\n",
    "            precision_scores.append(precision)\n",
    "            recall_scores.append(recall)\n",
    "            f1_scores.append(f1)\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}] \"\n",
    "                f\"Train Loss: {train_loss:.4f} \"\n",
    "                f\"Val Loss: {val_loss:.4f} \"\n",
    "                f\"Precision: {precision:.4f} \"\n",
    "                f\"Recall: {recall:.4f} \"\n",
    "                f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    return train_losses, val_losses, precision_scores, recall_scores, f1_scores\n",
    "\n",
    "\n",
    "model = CustomDeepClassifier(input_dim=train_dataset[0][0].shape[1], num_classes=20)\n",
    "train_losses, val_losses, precision_scores, recall_scores, f1_scores = train_validate_model(model, train_data_loader, test_data_loader, num_epochs=300, learning_rate=0.00005, stepslr=50, gamma=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14528/14528 [17:04<00:00, 14.18it/s] \n",
      "100%|██████████| 3280/3280 [03:46<00:00, 14.51it/s]\n",
      "100%|██████████| 14528/14528 [25:30<00:00,  9.49it/s] \n",
      "100%|██████████| 3280/3280 [03:53<00:00, 14.02it/s]\n",
      "100%|██████████| 14528/14528 [22:48<00:00, 10.62it/s] \n",
      "100%|██████████| 3280/3280 [03:42<00:00, 14.77it/s]\n",
      "100%|██████████| 14528/14528 [22:56<00:00, 10.55it/s] \n",
      "100%|██████████| 3280/3280 [04:15<00:00, 12.82it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def endpoint(input_csv, output_csv, output_folder, mode):\n",
    "    # Загрузка таблицы CSV\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    for index, row in tqdm(df.iterrows(), total=len(list(df.iterrows()))):\n",
    "        path_to_embeddings = row['path_to_embeddings']\n",
    "\n",
    "        # Загрузка тензора с эмбеддингами\n",
    "        embeddings_tensor = torch.load(path_to_embeddings)\n",
    "\n",
    "        # Получение первого и последнего эмбеддингов\n",
    "        first_embedding = embeddings_tensor[:, 0, :]\n",
    "        last_embedding = embeddings_tensor[:, -1, :]\n",
    "\n",
    "        # Конкатенация эмбеддингов\n",
    "        concatenated_embedding = torch.cat((first_embedding, last_embedding), dim=1)\n",
    "\n",
    "        # Формирование пути для сохранения нового эмбеддинга\n",
    "        new_embedding_path = f\"{output_folder}/embedding_{mode}_{index}.pt\"\n",
    "\n",
    "        # Сохранение нового эмбеддинга в формате .pt\n",
    "        torch.save(concatenated_embedding, new_embedding_path)\n",
    "\n",
    "        # Запись пути до сохраненного эмбеддинга в dataframe\n",
    "        df.at[index, 'path_to_embeddings'] = new_embedding_path\n",
    "\n",
    "    # Сохранение обновленного dataframe в CSV файл\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "# Пример вызова функции\n",
    "endpoint('bert-base-uncased_train_embeddings_with_span_raw.csv', 'bert-base-uncased_train_endpoint.csv', 'training/bert-base-uncased/endpoint', 'train')\n",
    "endpoint('bert-base-uncased_test_embeddings_with_span_raw.csv', 'bert-base-uncased_test_endpoint.csv', 'training/bert-base-uncased/endpoint', 'test')\n",
    "endpoint('roberta-base_train_embeddings_with_span_raw.csv', 'bert-base-uncased_train_endpoint.csv', 'training/roberta-base/endpoint', 'train')\n",
    "endpoint('roberta-base_test_embeddings_with_span_raw.csv', 'bert-base-uncased_test_endpoint.csv', 'training/roberta-base/endpoint', 'test')\n",
    "endpoint('xlnet-base-cased_train_embeddings_with_span_raw.csv', 'bert-base-uncased_train_endpoint.csv', 'training/xlnet-base-cased/endpoint', 'train')\n",
    "endpoint('xlnet-base-cased_test_embeddings_with_span_raw.csv', 'bert-base-uncased_test_endpoint.csv', 'training/xlnet-base-cased/endpoint', 'test')\n",
    "endpoint('SpanBERT/spanbert-base-cased_train_embeddings_with_span_raw.csv', 'bert-base-uncased_train_endpoint.csv', 'training/spanbert-base-cased/endpoint', 'train')\n",
    "endpoint('SpanBERT/spanbert-base-cased_test_embeddings_with_span_raw.csv', 'bert-base-uncased_test_endpoint.csv', 'training/spanbert-base-cased/endpoint', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14528/14528 [21:35<00:00, 11.21it/s]\n",
      "100%|██████████| 3280/3280 [04:07<00:00, 13.27it/s]\n",
      "100%|██████████| 14528/14528 [27:57<00:00,  8.66it/s] \n",
      "100%|██████████| 3280/3280 [04:55<00:00, 11.12it/s]\n",
      "100%|██████████| 14528/14528 [22:24<00:00, 10.80it/s] \n",
      "100%|██████████| 3280/3280 [04:07<00:00, 13.23it/s]\n",
      "100%|██████████| 14528/14528 [22:50<00:00, 10.60it/s] \n",
      "100%|██████████| 3280/3280 [07:24<00:00,  7.38it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def diffsum(input_csv, output_csv, output_folder, mode):\n",
    "    # Загрузка таблицы CSV\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    for index, row in tqdm(df.iterrows(), total=len(list(df.iterrows()))):\n",
    "        path_to_embeddings = row['path_to_embeddings']\n",
    "\n",
    "        # Загрузка тензора с эмбеддингами\n",
    "        embeddings_tensor = torch.load(path_to_embeddings)\n",
    "\n",
    "        # Получение первого и последнего эмбеддингов\n",
    "        first_embedding = embeddings_tensor[:, 0, :]\n",
    "        last_embedding = embeddings_tensor[:, -1, :]\n",
    "\n",
    "        # Конкатенация эмбеддингов\n",
    "        concatenated_embedding = torch.cat((first_embedding + last_embedding, first_embedding - last_embedding), dim=1)\n",
    "\n",
    "        # Формирование пути для сохранения нового эмбеддинга\n",
    "        new_embedding_path = f\"{output_folder}/embedding_{mode}_{index}.pt\"\n",
    "\n",
    "        # Сохранение нового эмбеддинга в формате .pt\n",
    "        torch.save(concatenated_embedding, new_embedding_path)\n",
    "\n",
    "        # Запись пути до сохраненного эмбеддинга в dataframe\n",
    "        df.at[index, 'path_to_embeddings'] = new_embedding_path\n",
    "\n",
    "    # Сохранение обновленного dataframe в CSV файл\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "# Пример вызова функции\n",
    "diffsum('bert-base-uncased_train_embeddings_with_span_raw.csv', 'bert-base-uncased_train_diff-sumt.csv', 'training/bert-base-uncased/diff-sum', 'train')\n",
    "diffsum('bert-base-uncased_test_embeddings_with_span_raw.csv', 'bert-base-uncased_test_diff-sum.csv', 'training/bert-base-uncased/diff-sum', 'test')\n",
    "diffsum('roberta-base_train_embeddings_with_span_raw.csv', 'bert-base-uncased_train_diff-sum.csv', 'training/roberta-base/diff-sum', 'train')\n",
    "diffsum('roberta-base_test_embeddings_with_span_raw.csv', 'bert-base-uncased_test_diff-sum.csv', 'training/roberta-base/diff-sum', 'test')\n",
    "diffsum('xlnet-base-cased_train_embeddings_with_span_raw.csv', 'bert-base-uncased_train_diff-sum.csv', 'training/xlnet-base-cased/diff-sum', 'train')\n",
    "diffsum('xlnet-base-cased_test_embeddings_with_span_raw.csv', 'bert-base-uncased_test_diff-sum.csv', 'training/xlnet-base-cased/diff-sum', 'test')\n",
    "diffsum('SpanBERT/spanbert-base-cased_train_embeddings_with_span_raw.csv', 'bert-base-uncased_train_diff-sum.csv', 'training/spanbert-base-cased/diff-sum', 'train')\n",
    "diffsum('SpanBERT/spanbert-base-cased_test_embeddings_with_span_raw.csv', 'bert-base-uncased_test_diff-sum.csv', 'training/spanbert-base-cased/diff-sum', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14528/14528 [21:32<00:00, 11.24it/s]\n",
      "100%|██████████| 3280/3280 [04:16<00:00, 12.79it/s]\n",
      "100%|██████████| 14528/14528 [22:50<00:00, 10.60it/s] \n",
      "100%|██████████| 3280/3280 [04:11<00:00, 13.03it/s]\n",
      "100%|██████████| 14528/14528 [21:54<00:00, 11.05it/s] \n",
      "100%|██████████| 3280/3280 [04:05<00:00, 13.36it/s]\n",
      "100%|██████████| 14528/14528 [24:09<00:00, 10.03it/s] \n",
      "100%|██████████| 3280/3280 [04:11<00:00, 13.03it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def coherent(input_csv, output_csv, output_folder, mode):\n",
    "    # Загрузка таблицы CSV\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    for index, row in tqdm(df.iterrows(), total=len(list(df.iterrows()))):\n",
    "        path_to_embeddings = row['path_to_embeddings']\n",
    "\n",
    "        # Загрузка тензора с эмбеддингами\n",
    "        embeddings_tensor = torch.load(path_to_embeddings)\n",
    "\n",
    "        # Получение первого и последнего эмбеддингов\n",
    "        first_embedding = embeddings_tensor[:, 0, :]\n",
    "        last_embedding = embeddings_tensor[:, -1, :]\n",
    "\n",
    "        # 0.46875\n",
    "\n",
    "        n = int(round(first_embedding.shape[-1] * 0.46875))\n",
    "        e1 = first_embedding[:, :n]\n",
    "        e2 = last_embedding[:, :n]\n",
    "        e3 = first_embedding[:, n:]\n",
    "        e4 = last_embedding[:, n:]\n",
    "\n",
    "        # Конкатенация эмбеддингов\n",
    "        concatenated_embedding = torch.cat((e1, e2, torch.mul(e3, e4)), dim=1)\n",
    "\n",
    "        # Формирование пути для сохранения нового эмбеддинга\n",
    "        new_embedding_path = f\"{output_folder}/embedding_{mode}_{index}.pt\"\n",
    "\n",
    "        # Сохранение нового эмбеддинга в формате .pt\n",
    "        torch.save(concatenated_embedding, new_embedding_path)\n",
    "\n",
    "        # Запись пути до сохраненного эмбеддинга в dataframe\n",
    "        df.at[index, 'path_to_embeddings'] = new_embedding_path\n",
    "\n",
    "    # Сохранение обновленного dataframe в CSV файл\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "# Пример вызова функции\n",
    "coherent('bert-base-uncased_train_embeddings_with_span_raw.csv', 'bert-base-uncased_train_coherent.csv', 'training/bert-base-uncased/coherent', 'train')\n",
    "coherent('bert-base-uncased_test_embeddings_with_span_raw.csv', 'bert-base-uncased_test_coherent.csv', 'training/bert-base-uncased/coherent', 'test')\n",
    "coherent('roberta-base_train_embeddings_with_span_raw.csv', 'bert-base-uncased_train_coherent.csv', 'training/roberta-base/coherent', 'train')\n",
    "coherent('roberta-base_test_embeddings_with_span_raw.csv', 'bert-base-uncased_test_coherent.csv', 'training/roberta-base/coherent', 'test')\n",
    "coherent('xlnet-base-cased_train_embeddings_with_span_raw.csv', 'bert-base-uncased_train_coherent.csv', 'training/xlnet-base-cased/coherent', 'train')\n",
    "coherent('xlnet-base-cased_test_embeddings_with_span_raw.csv', 'bert-base-uncased_test_coherent.csv', 'training/xlnet-base-cased/coherent', 'test')\n",
    "coherent('SpanBERT/spanbert-base-cased_train_embeddings_with_span_raw.csv', 'bert-base-uncased_train_coherent.csv', 'training/spanbert-base-cased/coherent', 'train')\n",
    "coherent('SpanBERT/spanbert-base-cased_test_embeddings_with_span_raw.csv', 'bert-base-uncased_test_coherent.csv', 'training/spanbert-base-cased/coherent', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14528/14528 [23:35<00:00, 10.27it/s] \n",
      "100%|██████████| 3280/3280 [04:10<00:00, 13.09it/s]\n",
      "100%|██████████| 14528/14528 [24:52<00:00,  9.74it/s] \n",
      "100%|██████████| 3280/3280 [04:24<00:00, 12.40it/s]\n",
      "100%|██████████| 14528/14528 [22:21<00:00, 10.83it/s] \n",
      "100%|██████████| 3280/3280 [04:10<00:00, 13.07it/s]\n",
      "100%|██████████| 14528/14528 [21:51<00:00, 11.08it/s] \n",
      "100%|██████████| 3280/3280 [04:13<00:00, 12.94it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def maxpool(input_csv, output_csv, output_folder, mode):\n",
    "    # Загрузка таблицы CSV\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    for index, row in tqdm(df.iterrows(), total=len(list(df.iterrows()))):\n",
    "        path_to_embeddings = row['path_to_embeddings']\n",
    "\n",
    "        # Загрузка тензора с эмбеддингами\n",
    "        embeddings_tensor = torch.load(path_to_embeddings)\n",
    "\n",
    "        # Конкатенация эмбеддингов\n",
    "        concatenated_embedding = torch.max(embeddings_tensor, dim=1)[0]\n",
    "\n",
    "        # Формирование пути для сохранения нового эмбеддинга\n",
    "        new_embedding_path = f\"{output_folder}/embedding_{mode}_{index}.pt\"\n",
    "\n",
    "        # Сохранение нового эмбеддинга в формате .pt\n",
    "        torch.save(concatenated_embedding, new_embedding_path)\n",
    "\n",
    "        # Запись пути до сохраненного эмбеддинга в dataframe\n",
    "        df.at[index, 'path_to_embeddings'] = new_embedding_path\n",
    "\n",
    "    # Сохранение обновленного dataframe в CSV файл\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "# Пример вызова функции\n",
    "maxpool('bert-base-uncased_train_embeddings_with_span_raw.csv', 'bert-base-uncased_train_maxpooling.csv', 'training/bert-base-uncased/maxpooling', 'train')\n",
    "maxpool('bert-base-uncased_test_embeddings_with_span_raw.csv', 'bert-base-uncased_test_maxpooling.csv', 'training/bert-base-uncased/maxpooling', 'test')\n",
    "maxpool('roberta-base_train_embeddings_with_span_raw.csv', 'bert-base-uncased_train_maxpooling.csv', 'training/roberta-base/maxpooling', 'train')\n",
    "maxpool('roberta-base_test_embeddings_with_span_raw.csv', 'bert-base-uncased_test_maxpooling.csv', 'training/roberta-base/maxpooling', 'test')\n",
    "maxpool('xlnet-base-cased_train_embeddings_with_span_raw.csv', 'bert-base-uncased_train_maxpooling.csv', 'training/xlnet-base-cased/maxpooling', 'train')\n",
    "maxpool('xlnet-base-cased_test_embeddings_with_span_raw.csv', 'bert-base-uncased_test_maxpooling.csv', 'training/xlnet-base-cased/maxpooling', 'test')\n",
    "maxpool('SpanBERT/spanbert-base-cased_train_embeddings_with_span_raw.csv', 'bert-base-uncased_train_maxpooling.csv', 'training/spanbert-base-cased/maxpooling', 'train')\n",
    "maxpool('SpanBERT/spanbert-base-cased_test_embeddings_with_span_raw.csv', 'bert-base-uncased_test_maxpooling.csv', 'training/spanbert-base-cased/maxpooling', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14528/14528 [21:23<00:00, 11.32it/s] \n",
      "100%|██████████| 3280/3280 [04:06<00:00, 13.30it/s]\n",
      "100%|██████████| 14528/14528 [21:33<00:00, 11.24it/s] \n",
      "100%|██████████| 3280/3280 [04:05<00:00, 13.37it/s]\n",
      "100%|██████████| 14528/14528 [21:30<00:00, 11.26it/s]\n",
      "100%|██████████| 3280/3280 [04:10<00:00, 13.08it/s]\n",
      "100%|██████████| 14528/14528 [24:59<00:00,  9.69it/s] \n",
      "100%|██████████| 3280/3280 [05:00<00:00, 10.92it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def avgpool(input_csv, output_csv, output_folder, mode):\n",
    "    # Загрузка таблицы CSV\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    for index, row in tqdm(df.iterrows(), total=len(list(df.iterrows()))):\n",
    "        path_to_embeddings = row['path_to_embeddings']\n",
    "\n",
    "        # Загрузка тензора с эмбеддингами\n",
    "        embeddings_tensor = torch.load(path_to_embeddings)\n",
    "\n",
    "        # Конкатенация эмбеддингов\n",
    "        concatenated_embedding = torch.mean(embeddings_tensor, dim=1)\n",
    "\n",
    "        # Формирование пути для сохранения нового эмбеддинга\n",
    "        new_embedding_path = f\"{output_folder}/embedding_{mode}_{index}.pt\"\n",
    "\n",
    "        # Сохранение нового эмбеддинга в формате .pt\n",
    "        torch.save(concatenated_embedding, new_embedding_path)\n",
    "\n",
    "        # Запись пути до сохраненного эмбеддинга в dataframe\n",
    "        df.at[index, 'path_to_embeddings'] = new_embedding_path\n",
    "\n",
    "    # Сохранение обновленного dataframe в CSV файл\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "# Пример вызова функции\n",
    "avgpool('bert-base-uncased_train_embeddings_with_span_raw.csv', 'bert-base-uncased_train_avgpooling.csv', 'training/bert-base-uncased/avgpooling', 'train')\n",
    "avgpool('bert-base-uncased_test_embeddings_with_span_raw.csv', 'bert-base-uncased_test_avgpooling.csv', 'training/bert-base-uncased/avgpooling', 'test')\n",
    "avgpool('roberta-base_train_embeddings_with_span_raw.csv', 'bert-base-uncased_train_avgpooling.csv', 'training/roberta-base/avgpooling', 'train')\n",
    "avgpool('roberta-base_test_embeddings_with_span_raw.csv', 'bert-base-uncased_test_avgpooling.csv', 'training/roberta-base/avgpooling', 'test')\n",
    "avgpool('xlnet-base-cased_train_embeddings_with_span_raw.csv', 'bert-base-uncased_train_avgpooling.csv', 'training/xlnet-base-cased/avgpooling', 'train')\n",
    "avgpool('xlnet-base-cased_test_embeddings_with_span_raw.csv', 'bert-base-uncased_test_avgpooling.csv', 'training/xlnet-base-cased/avgpooling', 'test')\n",
    "avgpool('SpanBERT/spanbert-base-cased_train_embeddings_with_span_raw.csv', 'bert-base-uncased_train_avgpooling.csv', 'training/spanbert-base-cased/avgpooling', 'train')\n",
    "avgpool('SpanBERT/spanbert-base-cased_test_embeddings_with_span_raw.csv', 'bert-base-uncased_test_avgpooling.csv', 'training/spanbert-base-cased/avgpooling', 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, onehot_encoder):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.labels = onehot_encoder.transform(self.data['label'].values.reshape(-1, 1))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        file_path = self.data.iloc[idx]['path_to_embeddings']\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Load embedding from .pt file\n",
    "        embedding = torch.load(file_path)\n",
    "\n",
    "        # return embedding, torch.FloatTensor(label_onehot)\n",
    "        return embedding, torch.FloatTensor(label)\n",
    "\n",
    "\n",
    "class CustomDeepClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=1536, num_classes=20, num_layers=3, hidden_dim=1024):\n",
    "        super(CustomDeepClassifier, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        layers.append(nn.BatchNorm1d(1))\n",
    "        layers.append(nn.ReLU())\n",
    "\n",
    "        for _ in range(num_layers - 2):  # Subtract 2 for the initial and final layers\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.BatchNorm1d(1))\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "        layers.append(nn.Linear(hidden_dim, num_classes))\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        x = F.softmax(x, dim=0)\n",
    "        return x\n",
    "\n",
    "class CustomAttention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, proj_dim=256, num_layers=3, num_classes=20):\n",
    "        super(CustomAttention, self).__init__()\n",
    "\n",
    "        self.projection = nn.Linear(input_dim, proj_dim)\n",
    "\n",
    "        self.attention_params = nn.Linear(proj_dim, 1)\n",
    "\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(proj_dim, hidden_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.BatchNorm1d(1))\n",
    "        for _ in range(num_layers - 2):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.BatchNorm1d(1))\n",
    "        layers.append(nn.Linear(hidden_dim, num_classes))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, embeddings):\n",
    "        embeddings = self.projection(embeddings)\n",
    "\n",
    "        attn_logits = self.attention_params(embeddings)  #  + attention_mask\n",
    "        # print(attn_logits)\n",
    "        attention_wts = nn.functional.softmax(attn_logits, dim=2)\n",
    "        # print(attention_wts.shape, embeddings.shape)\n",
    "\n",
    "        attention_term = torch.sum(attention_wts * embeddings, dim=-1)  #  * attention_mask\n",
    "        # print(attention_term.shape)\n",
    "\n",
    "        output = self.layers(attention_term)\n",
    "        # print(output.shape)\n",
    "        \n",
    "        output = F.softmax(output, dim=0)\n",
    "        # print(output.shape)\n",
    "        \n",
    "        return output\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_validate_model(model, train_loader, val_loader, method, model_name, num_epochs=10, learning_rate=0.1, stepslr=10, gamma=0.9):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, stepslr, gamma=gamma)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), total=num_epochs):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs.float())\n",
    "            # print(outputs.shape, labels.shape)\n",
    "            # print(outputs.squeeze(1), labels)\n",
    "            loss = criterion(outputs.squeeze(1), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item()\n",
    "        \n",
    "        # print(running_train_loss, len(train_loader))\n",
    "        train_loss = running_train_loss / len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        if (epoch + 1) % 300 == 0:\n",
    "            model.eval()\n",
    "            running_val_loss = 0.0\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in val_loader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs = model(inputs.float())\n",
    "                    loss = criterion(outputs.squeeze(1), labels)\n",
    "                    running_val_loss += loss.item()\n",
    "\n",
    "                    # print(outputs.shape, labels.shape)\n",
    "                    # print(outputs, labels)\n",
    "                    predicted = torch.argmax(outputs.squeeze(1), 1)\n",
    "                    labels = torch.argmax(labels, 1)\n",
    "                    # print(predicted.shape, labels.shape)\n",
    "                    # print(predicted, labels)\n",
    "                    # break\n",
    "                    all_preds.extend(predicted.cpu().numpy())\n",
    "                    all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            val_loss = running_val_loss / len(val_loader)\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='micro')\n",
    "            precision_scores.append(precision)\n",
    "            recall_scores.append(recall)\n",
    "            f1_scores.append(f1)\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}] \"\n",
    "                f\"Train Loss: {train_loss:.4f} \"\n",
    "                f\"Val Loss: {val_loss:.4f} \"\n",
    "                f\"Precision: {precision:.4f} \"\n",
    "                f\"Recall: {recall:.4f} \"\n",
    "                f\"F1 Score: {f1:.4f}\")\n",
    "            \n",
    "            print(classification_report(all_labels, all_preds))\n",
    "            with open('metrics.txt', 'a') as file:\n",
    "                file.write(method + ' ' + model_name)\n",
    "                file.write(f'{precision}, {recall}, {f1}\\n')\n",
    "                file.write(classification_report(all_labels, all_preds) + '\\n')\n",
    "\n",
    "    return train_losses, val_losses, precision_scores, recall_scores, f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================================\n",
      " coherent bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [31:50<00:00,  6.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [300/300] Train Loss: 2.9032 Val Loss: 2.9881 Precision: 0.1302 Recall: 0.1302 F1 Score: 0.1302\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.02      0.14      0.03        21\n",
      "           1       0.05      0.04      0.04       133\n",
      "           2       0.01      0.14      0.01         7\n",
      "           3       0.02      0.11      0.03        35\n",
      "           4       0.02      0.10      0.03        20\n",
      "           5       0.00      0.04      0.01        26\n",
      "           6       0.17      0.06      0.09       188\n",
      "           7       0.09      0.07      0.08       121\n",
      "           8       0.04      0.16      0.06        61\n",
      "           9       0.23      0.14      0.17        95\n",
      "          10       0.00      0.00      0.00         4\n",
      "          11       0.64      0.22      0.32       569\n",
      "          12       0.56      0.15      0.24       296\n",
      "          13       0.72      0.12      0.21      1479\n",
      "          14       0.00      0.00      0.00        13\n",
      "          15       0.01      0.06      0.01        17\n",
      "          16       0.16      0.07      0.10       156\n",
      "          17       0.04      0.18      0.07        28\n",
      "          18       0.01      0.22      0.01         9\n",
      "          19       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.13      3280\n",
      "   macro avg       0.14      0.10      0.08      3280\n",
      "weighted avg       0.52      0.13      0.19      3280\n",
      "\n",
      "=============================================================\n",
      " coherent roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [31:51<00:00,  6.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [300/300] Train Loss: 2.9385 Val Loss: 2.9952 Precision: 0.0713 Recall: 0.0713 F1 Score: 0.0713\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.02      0.10      0.03        21\n",
      "           1       0.23      0.08      0.11       133\n",
      "           2       0.00      0.14      0.01         7\n",
      "           3       0.01      0.06      0.02        35\n",
      "           4       0.00      0.00      0.00        20\n",
      "           5       0.01      0.04      0.01        26\n",
      "           6       0.24      0.07      0.11       188\n",
      "           7       0.02      0.02      0.02       121\n",
      "           8       0.01      0.03      0.02        61\n",
      "           9       0.02      0.02      0.02        95\n",
      "          10       0.00      0.00      0.00         4\n",
      "          11       0.11      0.02      0.04       569\n",
      "          12       0.11      0.03      0.05       296\n",
      "          13       0.42      0.11      0.18      1479\n",
      "          14       0.01      0.08      0.01        13\n",
      "          15       0.00      0.00      0.00        17\n",
      "          16       0.04      0.04      0.04       156\n",
      "          17       0.02      0.07      0.03        28\n",
      "          18       0.00      0.00      0.00         9\n",
      "          19       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.07      3280\n",
      "   macro avg       0.06      0.05      0.03      3280\n",
      "weighted avg       0.24      0.07      0.11      3280\n",
      "\n",
      "=============================================================\n",
      " coherent xlnet-base-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [31:59<00:00,  6.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [300/300] Train Loss: 2.9352 Val Loss: 2.9899 Precision: 0.0646 Recall: 0.0646 F1 Score: 0.0646\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.02      0.05      0.02        21\n",
      "           1       0.08      0.04      0.05       133\n",
      "           2       0.00      0.00      0.00         7\n",
      "           3       0.00      0.03      0.00        35\n",
      "           4       0.00      0.00      0.00        20\n",
      "           5       0.04      0.12      0.06        26\n",
      "           6       0.12      0.03      0.05       188\n",
      "           7       0.06      0.03      0.04       121\n",
      "           8       0.06      0.08      0.07        61\n",
      "           9       0.17      0.06      0.09        95\n",
      "          10       0.00      0.00      0.00         4\n",
      "          11       0.33      0.08      0.12       569\n",
      "          12       0.28      0.07      0.11       296\n",
      "          13       0.52      0.06      0.11      1479\n",
      "          14       0.00      0.23      0.01        13\n",
      "          15       0.00      0.00      0.00        17\n",
      "          16       0.26      0.12      0.17       156\n",
      "          17       0.03      0.14      0.05        28\n",
      "          18       0.00      0.00      0.00         9\n",
      "          19       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.06      3280\n",
      "   macro avg       0.10      0.06      0.05      3280\n",
      "weighted avg       0.35      0.06      0.10      3280\n",
      "\n",
      "=============================================================\n",
      " coherent spanbert-base-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [32:55<00:00,  6.58s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [300/300] Train Loss: 2.9052 Val Loss: 2.9892 Precision: 0.0924 Recall: 0.0924 F1 Score: 0.0924\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.01      0.05      0.01        21\n",
      "           1       0.03      0.02      0.02       133\n",
      "           2       0.00      0.00      0.00         7\n",
      "           3       0.02      0.09      0.03        35\n",
      "           4       0.03      0.20      0.05        20\n",
      "           5       0.01      0.04      0.01        26\n",
      "           6       0.17      0.07      0.10       188\n",
      "           7       0.07      0.07      0.07       121\n",
      "           8       0.02      0.07      0.03        61\n",
      "           9       0.15      0.13      0.14        95\n",
      "          10       0.00      0.25      0.01         4\n",
      "          11       0.51      0.12      0.20       569\n",
      "          12       0.40      0.12      0.19       296\n",
      "          13       0.65      0.09      0.16      1479\n",
      "          14       0.01      0.08      0.01        13\n",
      "          15       0.00      0.00      0.00        17\n",
      "          16       0.06      0.04      0.05       156\n",
      "          17       0.01      0.07      0.02        28\n",
      "          18       0.00      0.11      0.01         9\n",
      "          19       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.09      3280\n",
      "   macro avg       0.11      0.08      0.06      3280\n",
      "weighted avg       0.44      0.09      0.14      3280\n",
      "\n",
      "=============================================================\n",
      " diff-sum bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [31:28<00:00,  6.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [300/300] Train Loss: 2.9048 Val Loss: 2.9842 Precision: 0.1643 Recall: 0.1643 F1 Score: 0.1643\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.04      0.19      0.06        21\n",
      "           1       0.10      0.08      0.09       133\n",
      "           2       0.00      0.00      0.00         7\n",
      "           3       0.01      0.11      0.03        35\n",
      "           4       0.00      0.00      0.00        20\n",
      "           5       0.02      0.08      0.03        26\n",
      "           6       0.22      0.11      0.14       188\n",
      "           7       0.18      0.11      0.13       121\n",
      "           8       0.08      0.21      0.12        61\n",
      "           9       0.21      0.15      0.17        95\n",
      "          10       0.01      0.25      0.01         4\n",
      "          11       0.66      0.23      0.34       569\n",
      "          12       0.51      0.15      0.23       296\n",
      "          13       0.72      0.17      0.28      1479\n",
      "          14       0.01      0.15      0.02        13\n",
      "          15       0.01      0.18      0.02        17\n",
      "          16       0.17      0.08      0.11       156\n",
      "          17       0.07      0.29      0.12        28\n",
      "          18       0.00      0.11      0.01         9\n",
      "          19       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.16      3280\n",
      "   macro avg       0.15      0.13      0.10      3280\n",
      "weighted avg       0.53      0.16      0.24      3280\n",
      "\n",
      "=============================================================\n",
      " diff-sum roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [35:27<00:00,  7.09s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [300/300] Train Loss: 2.9389 Val Loss: 2.9920 Precision: 0.0381 Recall: 0.0381 F1 Score: 0.0381\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.10      0.24      0.14        21\n",
      "           1       0.09      0.04      0.05       133\n",
      "           2       0.00      0.00      0.00         7\n",
      "           3       0.02      0.14      0.04        35\n",
      "           4       0.01      0.05      0.01        20\n",
      "           5       0.01      0.04      0.02        26\n",
      "           6       0.34      0.12      0.17       188\n",
      "           7       0.03      0.03      0.03       121\n",
      "           8       0.04      0.07      0.05        61\n",
      "           9       0.03      0.03      0.03        95\n",
      "          10       0.00      0.00      0.00         4\n",
      "          11       0.13      0.02      0.04       569\n",
      "          12       0.15      0.04      0.06       296\n",
      "          13       0.31      0.02      0.04      1479\n",
      "          14       0.00      0.08      0.01        13\n",
      "          15       0.01      0.18      0.02        17\n",
      "          16       0.10      0.06      0.07       156\n",
      "          17       0.01      0.11      0.02        28\n",
      "          18       0.00      0.11      0.00         9\n",
      "          19       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.04      3280\n",
      "   macro avg       0.07      0.07      0.04      3280\n",
      "weighted avg       0.21      0.04      0.05      3280\n",
      "\n",
      "=============================================================\n",
      " diff-sum xlnet-base-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [35:26<00:00,  7.09s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [300/300] Train Loss: 2.9095 Val Loss: 2.9862 Precision: 0.1445 Recall: 0.1445 F1 Score: 0.1445\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.04      0.10      0.06        21\n",
      "           1       0.13      0.05      0.07       133\n",
      "           2       0.00      0.00      0.00         7\n",
      "           3       0.01      0.09      0.02        35\n",
      "           4       0.01      0.05      0.02        20\n",
      "           5       0.00      0.00      0.00        26\n",
      "           6       0.22      0.05      0.09       188\n",
      "           7       0.11      0.06      0.08       121\n",
      "           8       0.00      0.00      0.00        61\n",
      "           9       0.28      0.12      0.16        95\n",
      "          10       0.00      0.25      0.01         4\n",
      "          11       0.70      0.20      0.31       569\n",
      "          12       0.50      0.12      0.19       296\n",
      "          13       0.68      0.18      0.29      1479\n",
      "          14       0.00      0.08      0.01        13\n",
      "          15       0.01      0.06      0.01        17\n",
      "          16       0.16      0.09      0.12       156\n",
      "          17       0.00      0.00      0.00        28\n",
      "          18       0.00      0.11      0.00         9\n",
      "          19       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.14      3280\n",
      "   macro avg       0.14      0.08      0.07      3280\n",
      "weighted avg       0.51      0.14      0.22      3280\n",
      "\n",
      "=============================================================\n",
      " diff-sum spanbert-base-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [33:15<00:00,  6.65s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [300/300] Train Loss: 2.9044 Val Loss: 2.9885 Precision: 0.0939 Recall: 0.0939 F1 Score: 0.0939\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.01      0.10      0.02        21\n",
      "           1       0.08      0.06      0.07       133\n",
      "           2       0.01      0.29      0.02         7\n",
      "           3       0.02      0.11      0.03        35\n",
      "           4       0.00      0.00      0.00        20\n",
      "           5       0.01      0.04      0.01        26\n",
      "           6       0.14      0.07      0.09       188\n",
      "           7       0.07      0.07      0.07       121\n",
      "           8       0.03      0.08      0.05        61\n",
      "           9       0.15      0.14      0.14        95\n",
      "          10       0.00      0.00      0.00         4\n",
      "          11       0.44      0.11      0.18       569\n",
      "          12       0.31      0.08      0.12       296\n",
      "          13       0.67      0.10      0.17      1479\n",
      "          14       0.01      0.23      0.03        13\n",
      "          15       0.01      0.18      0.02        17\n",
      "          16       0.08      0.06      0.07       156\n",
      "          17       0.01      0.04      0.02        28\n",
      "          18       0.01      0.11      0.02         9\n",
      "          19       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.09      3280\n",
      "   macro avg       0.10      0.09      0.06      3280\n",
      "weighted avg       0.43      0.09      0.14      3280\n",
      "\n",
      "=============================================================\n",
      " endpoint bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [36:02<00:00,  7.21s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [300/300] Train Loss: 2.9051 Val Loss: 2.9831 Precision: 0.1439 Recall: 0.1439 F1 Score: 0.1439\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.04      0.24      0.06        21\n",
      "           1       0.09      0.05      0.07       133\n",
      "           2       0.00      0.14      0.01         7\n",
      "           3       0.02      0.09      0.03        35\n",
      "           4       0.04      0.20      0.06        20\n",
      "           5       0.01      0.04      0.01        26\n",
      "           6       0.22      0.10      0.14       188\n",
      "           7       0.13      0.07      0.10       121\n",
      "           8       0.04      0.11      0.06        61\n",
      "           9       0.18      0.14      0.15        95\n",
      "          10       0.01      0.50      0.02         4\n",
      "          11       0.72      0.23      0.34       569\n",
      "          12       0.46      0.13      0.21       296\n",
      "          13       0.68      0.14      0.23      1479\n",
      "          14       0.01      0.15      0.02        13\n",
      "          15       0.01      0.12      0.01        17\n",
      "          16       0.24      0.14      0.18       156\n",
      "          17       0.06      0.18      0.10        28\n",
      "          18       0.01      0.22      0.01         9\n",
      "          19       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.14      3280\n",
      "   macro avg       0.15      0.15      0.09      3280\n",
      "weighted avg       0.51      0.14      0.21      3280\n",
      "\n",
      "=============================================================\n",
      " endpoint roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [35:57<00:00,  7.19s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [300/300] Train Loss: 2.9379 Val Loss: 2.9910 Precision: 0.0390 Recall: 0.0390 F1 Score: 0.0390\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.01      0.05      0.02        21\n",
      "           1       0.06      0.06      0.06       133\n",
      "           2       0.00      0.00      0.00         7\n",
      "           3       0.00      0.03      0.01        35\n",
      "           4       0.01      0.10      0.02        20\n",
      "           5       0.01      0.08      0.02        26\n",
      "           6       0.26      0.12      0.16       188\n",
      "           7       0.06      0.05      0.05       121\n",
      "           8       0.04      0.03      0.04        61\n",
      "           9       0.05      0.04      0.05        95\n",
      "          10       0.00      0.00      0.00         4\n",
      "          11       0.09      0.02      0.04       569\n",
      "          12       0.16      0.03      0.06       296\n",
      "          13       0.39      0.03      0.05      1479\n",
      "          14       0.00      0.08      0.00        13\n",
      "          15       0.01      0.12      0.01        17\n",
      "          16       0.11      0.06      0.08       156\n",
      "          17       0.05      0.11      0.07        28\n",
      "          18       0.01      0.11      0.01         9\n",
      "          19       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.04      3280\n",
      "   macro avg       0.07      0.06      0.04      3280\n",
      "weighted avg       0.24      0.04      0.06      3280\n",
      "\n",
      "=============================================================\n",
      " endpoint xlnet-base-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [35:47<00:00,  7.16s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [300/300] Train Loss: 2.9126 Val Loss: 2.9857 Precision: 0.1287 Recall: 0.1287 F1 Score: 0.1287\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        21\n",
      "           1       0.14      0.06      0.08       133\n",
      "           2       0.00      0.00      0.00         7\n",
      "           3       0.01      0.17      0.03        35\n",
      "           4       0.00      0.00      0.00        20\n",
      "           5       0.02      0.04      0.02        26\n",
      "           6       0.16      0.04      0.07       188\n",
      "           7       0.12      0.07      0.09       121\n",
      "           8       0.06      0.11      0.08        61\n",
      "           9       0.23      0.17      0.19        95\n",
      "          10       0.00      0.25      0.01         4\n",
      "          11       0.69      0.18      0.28       569\n",
      "          12       0.47      0.13      0.20       296\n",
      "          13       0.70      0.14      0.23      1479\n",
      "          14       0.00      0.15      0.01        13\n",
      "          15       0.02      0.24      0.04        17\n",
      "          16       0.20      0.12      0.15       156\n",
      "          17       0.00      0.00      0.00        28\n",
      "          18       0.00      0.11      0.01         9\n",
      "          19       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.13      3280\n",
      "   macro avg       0.14      0.10      0.07      3280\n",
      "weighted avg       0.51      0.13      0.20      3280\n",
      "\n",
      "=============================================================\n",
      " endpoint spanbert-base-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [34:22<00:00,  6.87s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [300/300] Train Loss: 2.9049 Val Loss: 2.9907 Precision: 0.0899 Recall: 0.0899 F1 Score: 0.0899\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.01      0.10      0.02        21\n",
      "           1       0.05      0.04      0.04       133\n",
      "           2       0.00      0.14      0.01         7\n",
      "           3       0.02      0.11      0.04        35\n",
      "           4       0.01      0.10      0.02        20\n",
      "           5       0.00      0.00      0.00        26\n",
      "           6       0.16      0.08      0.11       188\n",
      "           7       0.05      0.06      0.06       121\n",
      "           8       0.02      0.07      0.03        61\n",
      "           9       0.09      0.11      0.10        95\n",
      "          10       0.00      0.25      0.01         4\n",
      "          11       0.54      0.12      0.19       569\n",
      "          12       0.36      0.13      0.19       296\n",
      "          13       0.65      0.09      0.15      1479\n",
      "          14       0.01      0.23      0.02        13\n",
      "          15       0.00      0.06      0.01        17\n",
      "          16       0.05      0.03      0.03       156\n",
      "          17       0.01      0.04      0.02        28\n",
      "          18       0.00      0.11      0.01         9\n",
      "          19       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.09      3280\n",
      "   macro avg       0.10      0.09      0.05      3280\n",
      "weighted avg       0.44      0.09      0.14      3280\n",
      "\n",
      "=============================================================\n",
      " avgpooling bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [35:20<00:00,  7.07s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [300/300] Train Loss: 2.9130 Val Loss: 2.9849 Precision: 0.1311 Recall: 0.1311 F1 Score: 0.1311\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.07      0.14      0.09        21\n",
      "           1       0.11      0.03      0.05       133\n",
      "           2       0.01      0.14      0.01         7\n",
      "           3       0.02      0.09      0.03        35\n",
      "           4       0.04      0.10      0.06        20\n",
      "           5       0.02      0.08      0.03        26\n",
      "           6       0.28      0.07      0.11       188\n",
      "           7       0.14      0.04      0.06       121\n",
      "           8       0.06      0.03      0.04        61\n",
      "           9       0.22      0.11      0.14        95\n",
      "          10       0.01      0.25      0.02         4\n",
      "          11       0.57      0.25      0.34       569\n",
      "          12       0.37      0.06      0.11       296\n",
      "          13       0.63      0.13      0.22      1479\n",
      "          14       0.01      0.15      0.01        13\n",
      "          15       0.01      0.12      0.01        17\n",
      "          16       0.17      0.13      0.15       156\n",
      "          17       0.03      0.21      0.05        28\n",
      "          18       0.00      0.11      0.00         9\n",
      "          19       0.00      0.50      0.01         2\n",
      "\n",
      "    accuracy                           0.13      3280\n",
      "   macro avg       0.14      0.14      0.08      3280\n",
      "weighted avg       0.46      0.13      0.19      3280\n",
      "\n",
      "=============================================================\n",
      " avgpooling roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [35:37<00:00,  7.13s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [300/300] Train Loss: 2.9414 Val Loss: 2.9911 Precision: 0.1015 Recall: 0.1015 F1 Score: 0.1015\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.01      0.05      0.02        21\n",
      "           1       0.10      0.04      0.05       133\n",
      "           2       0.00      0.00      0.00         7\n",
      "           3       0.01      0.09      0.02        35\n",
      "           4       0.00      0.00      0.00        20\n",
      "           5       0.01      0.04      0.02        26\n",
      "           6       0.27      0.13      0.18       188\n",
      "           7       0.06      0.10      0.07       121\n",
      "           8       0.04      0.03      0.04        61\n",
      "           9       0.07      0.05      0.06        95\n",
      "          10       0.00      0.00      0.00         4\n",
      "          11       0.09      0.02      0.04       569\n",
      "          12       0.12      0.03      0.05       296\n",
      "          13       0.56      0.16      0.25      1479\n",
      "          14       0.01      0.08      0.01        13\n",
      "          15       0.00      0.06      0.00        17\n",
      "          16       0.15      0.06      0.08       156\n",
      "          17       0.04      0.29      0.06        28\n",
      "          18       0.01      0.22      0.02         9\n",
      "          19       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.10      3280\n",
      "   macro avg       0.08      0.07      0.05      3280\n",
      "weighted avg       0.31      0.10      0.15      3280\n",
      "\n",
      "=============================================================\n",
      " avgpooling xlnet-base-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [37:00<00:00,  7.40s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [300/300] Train Loss: 2.9152 Val Loss: 2.9868 Precision: 0.1396 Recall: 0.1396 F1 Score: 0.1396\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.02      0.05      0.03        21\n",
      "           1       0.17      0.08      0.11       133\n",
      "           2       0.01      0.14      0.02         7\n",
      "           3       0.01      0.09      0.01        35\n",
      "           4       0.04      0.10      0.06        20\n",
      "           5       0.00      0.00      0.00        26\n",
      "           6       0.19      0.06      0.09       188\n",
      "           7       0.13      0.06      0.08       121\n",
      "           8       0.04      0.03      0.04        61\n",
      "           9       0.24      0.12      0.16        95\n",
      "          10       0.02      0.50      0.03         4\n",
      "          11       0.60      0.23      0.33       569\n",
      "          12       0.29      0.05      0.09       296\n",
      "          13       0.59      0.15      0.24      1479\n",
      "          14       0.00      0.00      0.00        13\n",
      "          15       0.01      0.12      0.01        17\n",
      "          16       0.21      0.14      0.17       156\n",
      "          17       0.12      0.32      0.18        28\n",
      "          18       0.01      0.22      0.01         9\n",
      "          19       0.00      0.50      0.01         2\n",
      "\n",
      "    accuracy                           0.14      3280\n",
      "   macro avg       0.14      0.15      0.08      3280\n",
      "weighted avg       0.44      0.14      0.20      3280\n",
      "\n",
      "=============================================================\n",
      " avgpooling spanbert-base-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [35:10<00:00,  7.04s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [300/300] Train Loss: 2.9120 Val Loss: 2.9893 Precision: 0.0659 Recall: 0.0659 F1 Score: 0.0659\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.02      0.05      0.02        21\n",
      "           1       0.02      0.01      0.01       133\n",
      "           2       0.00      0.00      0.00         7\n",
      "           3       0.00      0.06      0.01        35\n",
      "           4       0.04      0.10      0.06        20\n",
      "           5       0.01      0.12      0.03        26\n",
      "           6       0.22      0.09      0.12       188\n",
      "           7       0.05      0.02      0.03       121\n",
      "           8       0.04      0.05      0.04        61\n",
      "           9       0.11      0.07      0.09        95\n",
      "          10       0.01      0.25      0.02         4\n",
      "          11       0.32      0.03      0.06       569\n",
      "          12       0.36      0.07      0.11       296\n",
      "          13       0.51      0.08      0.14      1479\n",
      "          14       0.00      0.00      0.00        13\n",
      "          15       0.01      0.12      0.02        17\n",
      "          16       0.14      0.10      0.11       156\n",
      "          17       0.02      0.07      0.03        28\n",
      "          18       0.00      0.00      0.00         9\n",
      "          19       0.00      0.50      0.00         2\n",
      "\n",
      "    accuracy                           0.07      3280\n",
      "   macro avg       0.09      0.09      0.05      3280\n",
      "weighted avg       0.35      0.07      0.10      3280\n",
      "\n",
      "=============================================================\n",
      " maxpooling bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [37:27<00:00,  7.49s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [300/300] Train Loss: 2.9170 Val Loss: 2.9871 Precision: 0.0832 Recall: 0.0832 F1 Score: 0.0832\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        21\n",
      "           1       0.06      0.02      0.02       133\n",
      "           2       0.00      0.00      0.00         7\n",
      "           3       0.01      0.20      0.03        35\n",
      "           4       0.03      0.05      0.03        20\n",
      "           5       0.05      0.19      0.08        26\n",
      "           6       0.22      0.04      0.07       188\n",
      "           7       0.14      0.07      0.09       121\n",
      "           8       0.09      0.08      0.09        61\n",
      "           9       0.22      0.12      0.15        95\n",
      "          10       0.01      0.25      0.02         4\n",
      "          11       0.35      0.18      0.24       569\n",
      "          12       0.40      0.06      0.11       296\n",
      "          13       0.47      0.05      0.09      1479\n",
      "          14       0.00      0.08      0.00        13\n",
      "          15       0.00      0.00      0.00        17\n",
      "          16       0.14      0.11      0.12       156\n",
      "          17       0.09      0.21      0.12        28\n",
      "          18       0.00      0.11      0.00         9\n",
      "          19       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.08      3280\n",
      "   macro avg       0.11      0.09      0.06      3280\n",
      "weighted avg       0.34      0.08      0.12      3280\n",
      "\n",
      "=============================================================\n",
      " maxpooling roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [37:27<00:00,  7.49s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [300/300] Train Loss: 2.9460 Val Loss: 2.9923 Precision: 0.0985 Recall: 0.0985 F1 Score: 0.0985\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        21\n",
      "           1       0.04      0.02      0.02       133\n",
      "           2       0.02      0.14      0.04         7\n",
      "           3       0.00      0.00      0.00        35\n",
      "           4       0.02      0.05      0.03        20\n",
      "           5       0.03      0.04      0.03        26\n",
      "           6       0.33      0.07      0.12       188\n",
      "           7       0.06      0.35      0.11       121\n",
      "           8       0.06      0.03      0.04        61\n",
      "           9       0.11      0.04      0.06        95\n",
      "          10       0.00      0.00      0.00         4\n",
      "          11       0.18      0.03      0.05       569\n",
      "          12       0.07      0.03      0.04       296\n",
      "          13       0.44      0.14      0.21      1479\n",
      "          14       0.00      0.00      0.00        13\n",
      "          15       0.00      0.00      0.00        17\n",
      "          16       0.05      0.10      0.07       156\n",
      "          17       0.04      0.25      0.06        28\n",
      "          18       0.00      0.00      0.00         9\n",
      "          19       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.10      3280\n",
      "   macro avg       0.07      0.06      0.04      3280\n",
      "weighted avg       0.26      0.10      0.13      3280\n",
      "\n",
      "=============================================================\n",
      " maxpooling xlnet-base-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [38:25<00:00,  7.68s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [300/300] Train Loss: 2.9192 Val Loss: 2.9857 Precision: 0.1018 Recall: 0.1018 F1 Score: 0.1018\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.02      0.05      0.03        21\n",
      "           1       0.10      0.03      0.05       133\n",
      "           2       0.00      0.00      0.00         7\n",
      "           3       0.02      0.20      0.04        35\n",
      "           4       0.00      0.00      0.00        20\n",
      "           5       0.01      0.04      0.02        26\n",
      "           6       0.23      0.05      0.08       188\n",
      "           7       0.08      0.02      0.04       121\n",
      "           8       0.08      0.08      0.08        61\n",
      "           9       0.26      0.13      0.17        95\n",
      "          10       0.02      0.50      0.05         4\n",
      "          11       0.55      0.14      0.23       569\n",
      "          12       0.38      0.06      0.11       296\n",
      "          13       0.52      0.10      0.17      1479\n",
      "          14       0.01      0.31      0.02        13\n",
      "          15       0.01      0.06      0.01        17\n",
      "          16       0.17      0.18      0.17       156\n",
      "          17       0.02      0.11      0.04        28\n",
      "          18       0.00      0.22      0.01         9\n",
      "          19       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.10      3280\n",
      "   macro avg       0.12      0.11      0.06      3280\n",
      "weighted avg       0.40      0.10      0.15      3280\n",
      "\n",
      "=============================================================\n",
      " maxpooling spanbert-base-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [35:33<00:00,  7.11s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [300/300] Train Loss: 2.9197 Val Loss: 2.9888 Precision: 0.0625 Recall: 0.0625 F1 Score: 0.0625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        21\n",
      "           1       0.06      0.02      0.02       133\n",
      "           2       0.00      0.00      0.00         7\n",
      "           3       0.01      0.06      0.01        35\n",
      "           4       0.06      0.15      0.08        20\n",
      "           5       0.02      0.19      0.04        26\n",
      "           6       0.19      0.03      0.05       188\n",
      "           7       0.09      0.03      0.05       121\n",
      "           8       0.04      0.03      0.04        61\n",
      "           9       0.18      0.08      0.11        95\n",
      "          10       0.00      0.00      0.00         4\n",
      "          11       0.39      0.07      0.11       569\n",
      "          12       0.38      0.07      0.11       296\n",
      "          13       0.43      0.06      0.11      1479\n",
      "          14       0.00      0.15      0.01        13\n",
      "          15       0.01      0.12      0.03        17\n",
      "          16       0.07      0.10      0.08       156\n",
      "          17       0.04      0.14      0.06        28\n",
      "          18       0.00      0.11      0.00         9\n",
      "          19       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.06      3280\n",
      "   macro avg       0.10      0.07      0.05      3280\n",
      "weighted avg       0.32      0.06      0.09      3280\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as outp:\n",
    "        pickle.dump(obj, outp, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "for method in ['coherent', 'diff-sum', 'endpoint', 'avgpooling', 'maxpooling']:\n",
    "    for model_name in ['bert-base-uncased', 'roberta-base', 'xlnet-base-cased', 'spanbert-base-cased']:\n",
    "        print('=============================================================\\n', method, model_name)\n",
    "        onehot_encoder = OneHotEncoder(sparse=False)\n",
    "        data = pd.read_csv(f'{model_name}_train_{method}.csv')['label'].values.reshape(-1, 1)\n",
    "        onehot_encoder.fit(data)\n",
    "            \n",
    "        # Paths to train and test CSV files\n",
    "        train_csv_path = f'{model_name}_train_{method}.csv'\n",
    "        test_csv_path = f'{model_name}_test_{method}.csv'\n",
    "\n",
    "        # Create instances of CustomDataset for train and test\n",
    "        train_dataset = CustomDataset(train_csv_path, onehot_encoder)\n",
    "        test_dataset = CustomDataset(test_csv_path, onehot_encoder)\n",
    "\n",
    "        # Define batch sizes\n",
    "        train_batch_size = 128\n",
    "        test_batch_size = 128\n",
    "\n",
    "        # Create DataLoaders for train and test datasets\n",
    "        train_data_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "        test_data_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "        model = CustomDeepClassifier(input_dim=train_dataset[0][0].shape[1], num_classes=20)\n",
    "        train_losses, val_losses, precision_scores, recall_scores, f1_scores = train_validate_model(model,\n",
    "                                                                                                    train_data_loader,\n",
    "                                                                                                    test_data_loader,\n",
    "                                                                                                    method,\n",
    "                                                                                                    model_name,\n",
    "                                                                                                    num_epochs=300,\n",
    "                                                                                                    learning_rate=3e-4,\n",
    "                                                                                                    stepslr=20,\n",
    "                                                                                                    gamma=0.9)\n",
    "        \n",
    "        save_object((train_losses, val_losses, precision_scores, recall_scores, f1_scores), f'{model_name}_{method}_data_and_metrics.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
