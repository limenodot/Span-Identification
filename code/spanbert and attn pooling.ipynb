{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertModel, AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check if GPU is available and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "mode = 'train'\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(f'semeval-{mode}-spans.csv')\n",
    "\n",
    "# Initialize the BERT tokenizer and model and move to GPU\n",
    "model_name = 'SpanBERT/spanbert-base-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, force_download=True)\n",
    "model = BertModel.from_pretrained(model_name, add_pooling_layer=False).to(device)\n",
    "\n",
    "# Function to process text and generate embeddings based on text spans\n",
    "def process_text_and_save_embeddings(text_path, text_span, index, model_name, mode):\n",
    "    # Load text from file using the path\n",
    "    with open(text_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Tokenize text and span\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    span_tokens = tokenizer.tokenize(text_span)\n",
    "\n",
    "    # Convert tokens to token IDs\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    span_token_ids = tokenizer.convert_tokens_to_ids(span_tokens)\n",
    "\n",
    "    # Find the start and end token indexes of the span in the text tokens\n",
    "    span_start, span_end = 0, 0\n",
    "    for i in range(len(token_ids) - len(span_token_ids) + 1):\n",
    "        if token_ids[i:i + len(span_token_ids)] == span_token_ids:\n",
    "            span_start, span_end = i, i + len(span_token_ids) - 1\n",
    "            break\n",
    "\n",
    "    # print(span_start, span_end)\n",
    "\n",
    "    # Split tokens into chunks with specified length and overlap\n",
    "    chunk_length = 510\n",
    "    overlap = 100\n",
    "    token_chunks = [[101] + token_ids[i:i + chunk_length] + [102] for i in range(0, len(token_ids), chunk_length - overlap)]\n",
    "\n",
    "    # Process each chunk and get embeddings for the specified span\n",
    "    embeddings = []\n",
    "    for i, chunk in enumerate(token_chunks):\n",
    "        input_ids = torch.tensor(chunk).unsqueeze(0).to(device)\n",
    "\n",
    "        # Get embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids)\n",
    "            last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        # Get CLS embedding\n",
    "        if i == 0:\n",
    "            cls_embedding = last_hidden_states[:, 0, :]\n",
    "\n",
    "        # Remove special tokens and concatenate remaining embeddings\n",
    "        trimmed_embeddings = last_hidden_states[:, 1:len(chunk) + 1, :]\n",
    "        if i < len(token_chunks) - 1:\n",
    "            trimmed_embeddings = trimmed_embeddings[:, overlap:, :]\n",
    "\n",
    "        embeddings.append(trimmed_embeddings)\n",
    "\n",
    "    # Concatenate embeddings from all chunks into one tensor\n",
    "    concatenated_embeddings = torch.cat(embeddings, dim=1)\n",
    "    concatenated_embeddings = concatenated_embeddings[:, span_start:span_end + 1, :]\n",
    "    concatenated_embeddings = torch.cat((cls_embedding.unsqueeze(1), concatenated_embeddings), dim=1)\n",
    "\n",
    "    # Save the tensor to a file in the 'embeddings' folder\n",
    "    embeddings_folder = f'{model_name}_{mode}_embeddings/'\n",
    "    os.makedirs(embeddings_folder, exist_ok=True)  # Create 'embeddings' folder if it doesn't exist\n",
    "    embeddings_filename = os.path.basename(text_path).replace('.txt', f'_{index}_span_raw_with_cls.pt')\n",
    "    embeddings_path = os.path.join(embeddings_folder, embeddings_filename)\n",
    "    torch.save(concatenated_embeddings, embeddings_path)\n",
    "\n",
    "    return embeddings_path\n",
    "\n",
    "# Process unique paths and save span embeddings\n",
    "embeddings_info = []\n",
    "for index, row in tqdm(data.iterrows(), total=len(data), desc='Processing spans'):\n",
    "    text_path = row['path']  # Update this with the actual folder path\n",
    "    text_span = row['span']  # Assuming 'span' column contains text-based spans like 'start-end'\n",
    "    embeddings_path = process_text_and_save_embeddings(text_path, text_span, index, model_name, mode)\n",
    "    embeddings_info.append({'path_to_text': text_path, 'path_to_embeddings': embeddings_path, 'label': row['label']})\n",
    "\n",
    "# Create DataFrame with paths to text and embeddings\n",
    "embeddings_df = pd.DataFrame(embeddings_info)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "embeddings_info_path = f'{model_name}_{mode}_embeddings_with_cls.csv'\n",
    "embeddings_df.to_csv(embeddings_info_path, index=False)\n",
    "\n",
    "\n",
    "mode = 'test'\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(f'semeval-{mode}-spans.csv')\n",
    "\n",
    "# Process unique paths and save span-max-pooled embeddings\n",
    "embeddings_info = []\n",
    "for index, row in tqdm(data.iterrows(), total=len(data), desc='Processing spans'):\n",
    "    text_path = row['path']  # Update this with the actual folder path\n",
    "    text_span = row['span']  # Assuming 'span' column contains text-based spans like 'start-end'\n",
    "    embeddings_path = process_text_and_save_embeddings(text_path, text_span, index, model_name, mode)\n",
    "    embeddings_info.append({'path_to_text': text_path, 'path_to_embeddings': embeddings_path, 'label': row['label']})\n",
    "\n",
    "# Create DataFrame with paths to text and embeddings\n",
    "embeddings_df = pd.DataFrame(embeddings_info)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "embeddings_info_path = f'{model_name}_{mode}_embeddings_with_cls.csv'\n",
    "embeddings_df.to_csv(embeddings_info_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\OneDrive\\Documents\\ДИССЕРТАЦИЯ\\Span Identification Project\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "data = pd.read_csv('SpanBERT/spanbert-base-cased_train_embeddings_with_cls.csv')['label'].values.reshape(-1, 1)\n",
    "onehot_encoder.fit(data)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, onehot_encoder, proj_dim=256, padding_size=256):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.labels = onehot_encoder.transform(self.data['label'].values.reshape(-1, 1))\n",
    "        self.padding_size = padding_size\n",
    "        self.proj_dim = proj_dim\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        file_path = self.data.iloc[idx]['path_to_embeddings']\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Load embedding from .pt file\n",
    "        embedding = torch.load(file_path).cpu()[:, 1:, :]\n",
    "\n",
    "        # Get attention mask\n",
    "        # attention_mask = torch.cat((\n",
    "        #     torch.ones((self.proj_dim, embedding.shape[1])),\n",
    "        #     torch.zeros((self.proj_dim, self.padding_size - embedding.shape[1]))\n",
    "        # ), dim=1)\n",
    "\n",
    "        padding = torch.zeros(1, self.padding_size - embedding.shape[1], embedding.shape[-1])\n",
    "        embedding = torch.cat((embedding, padding), dim=1)\n",
    "\n",
    "        return embedding, torch.FloatTensor(label)  # , torch.FloatTensor(attention_mask)\n",
    "    \n",
    "# Paths to train and test CSV files\n",
    "train_csv_path = 'SpanBERT/spanbert-base-cased_train_embeddings_with_cls.csv'\n",
    "test_csv_path = 'SpanBERT/spanbert-base-cased_test_embeddings_with_cls.csv'\n",
    "\n",
    "# Create instances of CustomDataset for train and test\n",
    "train_dataset = CustomDataset(train_csv_path, onehot_encoder)\n",
    "test_dataset = CustomDataset(test_csv_path, onehot_encoder)\n",
    "\n",
    "# Define batch sizes\n",
    "train_batch_size = 64\n",
    "test_batch_size = 64\n",
    "\n",
    "# Create DataLoaders for train and test datasets\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_validate_model(model, train_loader, val_loader, num_epochs=10, learning_rate=0.1, stepslr=10, gamma=0.9):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, stepslr, gamma=gamma)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), total=num_epochs):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        # for inputs, labels, attn_mask in tqdm(train_loader, desc=f'Training   {epoch+1}/{num_epochs}'):\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs.float())\n",
    "            # print(outputs.shape, labels.shape)\n",
    "            # print(outputs)\n",
    "        #     break\n",
    "        # break\n",
    "            # print(outputs.squeeze(1), labels)\n",
    "            loss = criterion(outputs.squeeze(1), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item()\n",
    "        \n",
    "        # print(running_train_loss, len(train_loader))\n",
    "        train_loss = running_train_loss / len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            model.eval()\n",
    "            running_val_loss = 0.0\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in val_loader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs = model(inputs.float())\n",
    "                    loss = criterion(outputs.squeeze(1), labels)\n",
    "                    running_val_loss += loss.item()\n",
    "\n",
    "                    # print(outputs.shape, labels.shape)\n",
    "                    # print(outputs, labels)\n",
    "                    predicted = torch.argmax(outputs.squeeze(1), -1)\n",
    "                    labels = torch.argmax(labels, 1)\n",
    "                    # print(predicted.shape, labels.shape)\n",
    "                    # print(predicted, labels)\n",
    "                    # break\n",
    "                    all_preds.extend(predicted.cpu().numpy())\n",
    "                    all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            val_loss = running_val_loss / len(val_loader)\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "            # print(all_preds, all_labels)\n",
    "            # break\n",
    "\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='macro')\n",
    "            precision_scores.append(precision)\n",
    "            recall_scores.append(recall)\n",
    "            f1_scores.append(f1)\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}] \"\n",
    "                f\"Train Loss: {train_loss:.4f} \"\n",
    "                f\"Val Loss: {val_loss:.4f} \"\n",
    "                f\"Precision: {precision:.4f} \"\n",
    "                f\"Recall: {recall:.4f} \"\n",
    "                f\"F1 Score: {f1:.4f}\")\n",
    "            print(classification_report(all_labels, all_preds))\n",
    "\n",
    "    return train_losses, val_losses, precision_scores, recall_scores, f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomAttention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, proj_dim=256, num_layers=3, num_classes=20):\n",
    "        super(CustomAttention, self).__init__()\n",
    "\n",
    "        self.projection = nn.Linear(input_dim, proj_dim)\n",
    "\n",
    "        self.attention_params = nn.Linear(proj_dim, 1)\n",
    "\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(proj_dim, hidden_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.BatchNorm1d(1))\n",
    "        for _ in range(num_layers - 2):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.BatchNorm1d(1))\n",
    "        layers.append(nn.Linear(hidden_dim, num_classes))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, embeddings):\n",
    "        embeddings = self.projection(embeddings)\n",
    "\n",
    "        attn_logits = self.attention_params(embeddings)  #  + attention_mask\n",
    "        # print(attn_logits)\n",
    "        attention_wts = nn.functional.softmax(attn_logits, dim=2)\n",
    "        # print(attention_wts.shape, embeddings.shape)\n",
    "\n",
    "        attention_term = torch.sum(attention_wts * embeddings, dim=-1)  #  * attention_mask\n",
    "        # print(attention_term.shape)\n",
    "\n",
    "        output = self.layers(attention_term)\n",
    "        # print(output.shape)\n",
    "        \n",
    "        output = F.softmax(output, dim=0)\n",
    "        # print(output.shape)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 100/300 [19:56<1:04:55, 19.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/300] Train Loss: 2.9090 Val Loss: 2.9956 Precision: 0.0615 Recall: 0.0540 F1 Score: 0.0324\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.01      0.05      0.01        21\n",
      "           1       0.05      0.05      0.05       133\n",
      "           2       0.00      0.00      0.00         7\n",
      "           3       0.01      0.11      0.02        35\n",
      "           4       0.02      0.15      0.04        20\n",
      "           5       0.02      0.12      0.03        26\n",
      "           6       0.08      0.07      0.08       188\n",
      "           7       0.01      0.01      0.01       121\n",
      "           8       0.05      0.11      0.07        61\n",
      "           9       0.02      0.03      0.02        95\n",
      "          10       0.00      0.00      0.00         4\n",
      "          11       0.39      0.08      0.13       569\n",
      "          12       0.13      0.04      0.06       296\n",
      "          13       0.37      0.04      0.07      1479\n",
      "          14       0.00      0.00      0.00        13\n",
      "          15       0.00      0.06      0.01        17\n",
      "          16       0.05      0.03      0.03       156\n",
      "          17       0.01      0.04      0.01        28\n",
      "          18       0.01      0.11      0.01         9\n",
      "          19       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.05      3280\n",
      "   macro avg       0.06      0.05      0.03      3280\n",
      "weighted avg       0.26      0.05      0.07      3280\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 200/300 [39:33<20:48, 12.49s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/300] Train Loss: 2.8997 Val Loss: 2.9961 Precision: 0.0623 Recall: 0.0462 F1 Score: 0.0329\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.01      0.10      0.02        21\n",
      "           1       0.04      0.05      0.04       133\n",
      "           2       0.00      0.00      0.00         7\n",
      "           3       0.01      0.06      0.02        35\n",
      "           4       0.01      0.10      0.02        20\n",
      "           5       0.01      0.08      0.02        26\n",
      "           6       0.11      0.11      0.11       188\n",
      "           7       0.03      0.02      0.03       121\n",
      "           8       0.05      0.13      0.07        61\n",
      "           9       0.02      0.02      0.02        95\n",
      "          10       0.00      0.00      0.00         4\n",
      "          11       0.30      0.04      0.07       569\n",
      "          12       0.13      0.06      0.08       296\n",
      "          13       0.45      0.06      0.10      1479\n",
      "          14       0.01      0.08      0.01        13\n",
      "          15       0.00      0.00      0.00        17\n",
      "          16       0.07      0.03      0.04       156\n",
      "          17       0.00      0.00      0.00        28\n",
      "          18       0.00      0.00      0.00         9\n",
      "          19       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.05      3280\n",
      "   macro avg       0.06      0.05      0.03      3280\n",
      "weighted avg       0.28      0.05      0.08      3280\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [58:59<00:00, 11.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [300/300] Train Loss: 2.8946 Val Loss: 2.9978 Precision: 0.0590 Recall: 0.0434 F1 Score: 0.0317\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.01      0.10      0.02        21\n",
      "           1       0.06      0.05      0.06       133\n",
      "           2       0.00      0.00      0.00         7\n",
      "           3       0.01      0.06      0.02        35\n",
      "           4       0.01      0.10      0.02        20\n",
      "           5       0.01      0.08      0.02        26\n",
      "           6       0.08      0.09      0.08       188\n",
      "           7       0.04      0.05      0.04       121\n",
      "           8       0.03      0.07      0.04        61\n",
      "           9       0.01      0.01      0.01        95\n",
      "          10       0.00      0.00      0.00         4\n",
      "          11       0.27      0.04      0.06       569\n",
      "          12       0.14      0.07      0.10       296\n",
      "          13       0.45      0.06      0.11      1479\n",
      "          14       0.01      0.08      0.02        13\n",
      "          15       0.00      0.00      0.00        17\n",
      "          16       0.05      0.03      0.03       156\n",
      "          17       0.00      0.00      0.00        28\n",
      "          18       0.00      0.00      0.00         9\n",
      "          19       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.06      3280\n",
      "   macro avg       0.06      0.04      0.03      3280\n",
      "weighted avg       0.27      0.06      0.08      3280\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = CustomAttention(input_dim=train_dataset[0][0].shape[-1], hidden_dim=256)\n",
    "train_losses, val_losses, precision_scores, recall_scores, f1_scores = train_validate_model(model, train_data_loader, test_data_loader, num_epochs=300, learning_rate=3e-4, stepslr=50, gamma=0.9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
